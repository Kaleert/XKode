#!/data/data/com.termux/files/usr/bin/python3
#!/data/data/com.termux/files/usr/bin/python3
import os
import yaml
import argparse
from pathlib import Path
import mimetypes
import fnmatch
from datetime import datetime
import re
import glob
import sys
import math
from collections import Counter
import ast
import tokenize
from io import StringIO

class CodeAnalyzer:
    def __init__(self, config_path="analyzer.yml"):
        self.config = self.read_config(config_path)
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'skipped_files': 0,
            'binary_files': 0,
            'excluded_files': 0,
            'error_files': 0,
            'total_lines': 0,
            'total_size_bytes': 0,
            'total_tokens': 0,
            'comment_chars': 0,
            'code_chars': 0,
            'empty_lines': 0,
            'import_lines': 0,
            'boilerplate_lines': 0,
            'useful_code_percent': 0,
            'files_by_language': {},
            'complex_files': [],
            'clean_removed_chars': 0,
            'all_todos': [],
            'unique_todos': set(),
            'method_count': 0,
            'class_count': 0,
            'avg_method_length': 0,
            'complexity_score': 0,
            'tech_debt_score': 0,
            'hotspots': []
        }
        self.todo_patterns = self.setup_todo_patterns()
        self.boilerplate_patterns = self.setup_boilerplate_patterns()
    
    def setup_todo_patterns(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ TODO –∏ –º–µ—Å—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è"""
        return {
            'todo_keywords': [
                r'TODO', r'FIXME', r'XXX', r'HACK', r'BUG', 
                r'NOTE', r'OPTIMIZE', r'REVIEW', r'IMPROVE'
            ],
            'improvement_phrases': [
                r'–Ω—É–∂–Ω–æ\s+(–¥–æ–¥–µ–ª–∞—Ç—å|—Å–¥–µ–ª–∞—Ç—å|—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å|–¥–æ–ø–∏—Å–∞—Ç—å)',
                r'—Å–ª–µ–¥—É–µ—Ç\s+(–¥–æ–¥–µ–ª–∞—Ç—å|—Å–¥–µ–ª–∞—Ç—å|—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å|—É–ª—É—á—à–∏—Ç—å)',
                r'–Ω–∞–¥–æ\s+(–¥–æ–¥–µ–ª–∞—Ç—å|—Å–¥–µ–ª–∞—Ç—å|—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å)',
                r'—Ç—Ä–µ–±—É–µ—Ç—Å—è\s+(–¥–æ–¥–µ–ª–∞—Ç—å|—Å–¥–µ–ª–∞—Ç—å|—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å)',
                r'—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å', r'–¥–æ–¥–µ–ª–∞—Ç—å', r'—Å–¥–µ–ª–∞—Ç—å',
                r'–≤\s+–±—É–¥—É—â–µ–º', r'–ø–æ–∑–∂–µ', r'–ø–æ—Ç–æ–º',
                r'–≤—Ä–µ–º–µ–Ω–Ω–æ–µ\s+—Ä–µ—à–µ–Ω–∏–µ', r'–≤—Ä–µ–º–µ–Ω–Ω—ã–π\s+–∫–æ–¥',
                r'–∫–æ—Å—Ç—ã–ª—å', r'–∑–∞–≥–ª—É—à–∫–∞', r'–∑–∞—Ç—ã—á–∫–∞',
                r'—É–ø—Ä–æ—â[–µ—ë]–Ω–Ω—ã–π', r'—É–ø—Ä–æ—â–µ–Ω–∏–µ',
                r'–≤\s+–ø—Ä–æ–¥–∞–∫—à–Ω–µ', r'–≤\s+–ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ',
                r'–ª—É—á—à–µ\s+—Å–¥–µ–ª–∞—Ç—å', r'–ª—É—á—à–µ\s+—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å',
                r'–Ω–µ\s+–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ', r'–Ω–µ\s+–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ',
                r'–º–µ–¥–ª–µ–Ω–Ω–æ\s+—Ä–∞–±–æ—Ç–∞–µ—Ç', r'—É—Å–∫–æ—Ä–∏—Ç—å',
                r'–ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å', r'—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥',
                r'–Ω–µ\s+—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ', r'–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å',
                r'–¥–æ–±–∞–≤–∏—Ç—å\s+–ø—Ä–æ–≤–µ—Ä–∫—É', r'–¥–æ–±–∞–≤–∏—Ç—å\s+–æ–±—Ä–∞–±–æ—Ç–∫—É',
                r'–Ω–µ\s+–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ', r'–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å\s+–æ—à–∏–±–∫—É',
                r'—É–ø—Ä–æ—Å—Ç–∏—Ç—å', r'—É–ª—É—á—à–∏—Ç—å',
                r'—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è\s+—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è', r'—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è\s+—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è',
                r'—É–ª—É—á—à–µ–Ω–Ω–∞—è\s+—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è', r'–ø–æ–ª–Ω–∞—è\s+—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è',
                r'–¥–æ–ø–æ–ª–Ω–∏—Ç—å', r'—Ä–∞—Å—à–∏—Ä–∏—Ç—å', r'–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å'
            ]
        }
    
    def setup_boilerplate_patterns(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è boilerplate –∫–æ–¥–∞"""
        return {
            'import_patterns': [
                r'^\s*import\s+', r'^\s*from\s+.*\s+import', r'^\s*#include',
                r'^\s*package\s+', r'^\s*using\s+', r'^\s*require\s+', r'^\s*#require',
            ],
            'boilerplate_patterns': [
                r'^\s*@(Override|Autowired|Service|Controller|Component|Repository|Entity|Table|Column|Id|GeneratedValue|Getter|Setter|Data|Builder|NoArgsConstructor|AllArgsConstructor)',
                r'^\s*(public|private|protected)\s+class', r'^\s*(public|private|protected)\s+interface',
                r'^\s*(public|private|protected)\s+enum', r'^\s*(public|private|protected)\s+static\s+final',
                r'^\s*public\s+\w+\s+get\w+\s*\(\s*\)', r'^\s*public\s+void\s+set\w+\s*\(\s*\w+\s+\w+\s*\)',
                r'^\s*public\s+\w+\s*\(\s*\)',
            ]
        }
    
    def read_config(self, config_path):
        """–ß–∏—Ç–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML —Ñ–∞–π–ª–∞"""
        if not os.path.exists(config_path):
            default_config = {
                'exclude': [
                    '**/node_modules/**', '**/__pycache__/**', '**/.git/**',
                    '**/venv/**', '**/env/**', '**/build/**', '**/dist/**',
                    '**/target/**', '**/*.min.js', '**/*.min.css'
                ],
                'include': [],
                'filename': 'project_analysis.txt',
                'separator': '/* --- %path% --- */',
                'skipBinary': True,
                'encoding': 'utf-8',
                'maxFileSize': 5,
                'includeExtensions': [],
                'excludeExtensions': ['.min.js', '.min.css', '.log', '.tmp', '.bak'],
                'addTimestamp': True,
                'addHeader': True,
                'lineEndings': 'preserve',
                'verbose': False,
                'splitIntoParts': 1,
                'maxTokensPerPart': 0,
                'countEmptyLines': True,
                'showFileStats': True,
                'scanTodos': True,
                'minCommentLength': 30,
                'showTodosInStats': True,
                'analyzeCodeQuality': True,
                'complexityThreshold': 100,
                'outputFormat': 'text',
                'showCommentStats': False,
                'exportTodos': True,
                'analyzeComplexity': True,
                'findHotspots': True,
                'techDebtAnalysis': True
            }
            
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(default_config, f, default_flow_style=False, allow_unicode=True)
            
            print(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {config_path}")
            return default_config
        
        with open(config_path, 'r', encoding='utf-8') as file:
            config = yaml.safe_load(file)
        
        defaults = {
            'exclude': [], 'include': [], 'filename': 'project_analysis.txt',
            'separator': '/* %path% */', 'skipBinary': True, 'encoding': 'utf-8',
            'maxFileSize': 0, 'includeExtensions': [], 'excludeExtensions': [],
            'addTimestamp': False, 'addHeader': False, 'lineEndings': 'preserve',
            'verbose': False, 'splitIntoParts': 1, 'maxTokensPerPart': 0,
            'countEmptyLines': True, 'showFileStats': True, 'scanTodos': True,
            'minCommentLength': 50, 'showTodosInStats': True, 'analyzeCodeQuality': True,
            'complexityThreshold': 100, 'outputFormat': 'text', 'showCommentStats': False,
            'exportTodos': True, 'analyzeComplexity': True, 'findHotspots': True,
            'techDebtAnalysis': True
        }
        
        for key, value in defaults.items():
            config.setdefault(key, value)
        
        return config
        
    def analyze_java_complexity(self, content, file_path):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å Java –∫–æ–¥–∞"""
        if not self.config.get('analyzeComplexity', True):
            return {}
        
        if self.detect_language(file_path) != 'java':
            return {}
        
        metrics = {
            'method_count': 0,
            'class_count': 0,
            'avg_method_lines': 0,
            'max_method_lines': 0,
            'complex_methods': []
        }
        
        # –ü–æ–∏—Å–∫ –∫–ª–∞—Å—Å–æ–≤
        class_pattern = r'class\s+(\w+)'
        metrics['class_count'] = len(re.findall(class_pattern, content))
        
        # –ü–æ–∏—Å–∫ –º–µ—Ç–æ–¥–æ–≤
        method_pattern = r'(public|private|protected)\s+(\w+(?:<\w+>)?)\s+(\w+)\s*\([^)]*\)\s*\{'
        methods = []
        
        for match in re.finditer(method_pattern, content):
            method_start = match.end()
            # –ù–∞—Ö–æ–¥–∏–º –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É –º–µ—Ç–æ–¥–∞
            brace_count = 1
            pos = method_start
            while brace_count > 0 and pos < len(content):
                if content[pos] == '{':
                    brace_count += 1
                elif content[pos] == '}':
                    brace_count -= 1
                pos += 1
            
            if brace_count == 0:
                method_content = content[method_start:pos-1]
                method_lines = method_content.count('\n') + 1
                methods.append({
                    'name': match.group(3),
                    'lines': method_lines,
                    'start_line': content[:match.start()].count('\n') + 1
                })
        
        metrics['method_count'] = len(methods)
        if methods:
            metrics['avg_method_lines'] = sum(m['lines'] for m in methods) / len(methods)
            metrics['max_method_lines'] = max(m['lines'] for m in methods)
            # –°–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã (>50 —Å—Ç—Ä–æ–∫)
            metrics['complex_methods'] = [m for m in methods if m['lines'] > 50]
        
        return metrics
        
    def find_code_hotspots(self, content, file_path):
        """–ù–∞—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–µ—Å—Ç–∞ –≤ –∫–æ–¥–µ"""
        if not self.config.get('findHotspots', True):
            return []
        
        hotspots = []
        lines = content.split('\n')
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –∫–æ–¥–∞
        problem_patterns = [
            (r'System\.out\.println', '–ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –≤ –∫–æ–¥–µ'),
            (r'e\.printStackTrace\(\)', '–í—ã–≤–æ–¥ stack trace'),
            (r'Thread\.sleep\(', '–ü—Ä—è–º–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–æ—Ç–æ–∫–∞'),
            (r'catch\s*\(\s*Exception\s+e\s*\)', '–®–∏—Ä–æ–∫–∏–π catch Exception'),
            (r'if\s*\([^)]*\)\s*\{[^}]{200,}', '–î–ª–∏–Ω–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ if'),
            (r'for\s*\([^)]*\)\s*\{[^}]{100,}', '–°–ª–æ–∂–Ω—ã–π —Ü–∏–∫–ª for'),
        ]
        
        for i, line in enumerate(lines, 1):
            for pattern, description in problem_patterns:
                if re.search(pattern, line):
                    hotspots.append({
                        'line': i,
                        'description': description,
                        'code': line.strip()[:100]
                    })
                    break
        
        return hotspots
        
    def calculate_tech_debt_score(self):
	    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–ª–≥–∞"""
	    if not self.config.get('techDebtAnalysis', True):
	        return 0
	    
	    score = 0
	    
	    # TODO –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
	    todo_count = sum(1 for todo in self.stats['all_todos'] if 'TODO' in todo['type'])
	    score += todo_count * 5
	    
	    # –°–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã
	    file_metrics = getattr(self.stats, 'file_metrics', {})
	    complex_methods_count = sum(len(metrics.get('complex_methods', [])) 
	                              for metrics in file_metrics.values())
	    score += complex_methods_count * 3
	    
	    # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–µ—Å—Ç–∞
	    hotspots_dict = getattr(self.stats, 'hotspots', {})
	    hotspots_count = sum(len(hotspots) for hotspots in hotspots_dict.values())
	    score += hotspots_count * 2
	    
	    # –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã
	    large_files = sum(1 for metrics in file_metrics.values() 
	                     if metrics.get('method_count', 0) > 20)
	    score += large_files * 4
	    
	    return min(score, 100)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 100 –±–∞–ª–ª–∞–º–∏
    
    def is_binary_file(self, file_path):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –±–∏–Ω–∞—Ä–Ω—ã–º –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏"""
        mime_type, _ = mimetypes.guess_type(file_path)
        if mime_type and not mime_type.startswith('text'):
            return True
        
        binary_extensions = {'.exe', '.dll', '.so', '.class', '.jar', '.zip', 
                           '.rar', '.tar', '.gz', '.png', '.jpg', '.jpeg', 
                           '.gif', '.bmp', '.ico', '.pdf', '.doc', '.docx'}
        if Path(file_path).suffix.lower() in binary_extensions:
            return True
        
        try:
            with open(file_path, 'tr', encoding='utf-8') as check_file:
                check_file.read(1024)
            return False
        except:
            return True
    
    def should_include_file(self, file_path, relative_path):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –≤–∫–ª—é—á–∞—Ç—å —Ñ–∞–π–ª –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        exclude_list = self.config.get('exclude', [])
        include_list = self.config.get('include', [])
        include_extensions = self.config.get('includeExtensions', [])
        exclude_extensions = self.config.get('excludeExtensions', [])
        
        if exclude_list:
            if any(fnmatch.fnmatch(relative_path, pattern) for pattern in exclude_list):
                return False
        
        if include_list:
            if not any(fnmatch.fnmatch(relative_path, pattern) for pattern in include_list):
                return False
        
        file_extension = Path(file_path).suffix.lower()
        
        if include_extensions and file_extension not in include_extensions:
            return False
        
        if exclude_extensions and file_extension in exclude_extensions:
            return False
        
        max_size = self.config.get('maxFileSize', 0)
        if max_size > 0:
            file_size = os.path.getsize(file_path)
            if file_size > max_size * 1024 * 1024:
                return False
        
        return True

    def detect_language(self, file_path):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Ñ–∞–π–ª–∞"""
        ext_to_lang = {
            '.java': 'java', '.js': 'javascript', '.jsx': 'javascript',
            '.ts': 'typescript', '.tsx': 'typescript', '.py': 'python',
            '.cpp': 'cpp', '.cc': 'cpp', '.cxx': 'cpp', '.c': 'c',
            '.h': 'c', '.hpp': 'cpp', '.html': 'html', '.htm': 'html',
            '.css': 'css', '.php': 'php', '.rb': 'ruby', '.go': 'go',
            '.rs': 'rust', '.swift': 'swift', '.kt': 'kotlin',
            '.scala': 'scala', '.sql': 'sql', '.xml': 'xml', '.json': 'json',
            '.yaml': 'yaml', '.yml': 'yaml', '.md': 'markdown',
            '.gradle': 'groovy', '.properties': 'properties'
        }
        return ext_to_lang.get(Path(file_path).suffix.lower(), 'unknown')

    def analyze_code_metrics(self, content, file_path):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–æ–¥–∞"""
        language = self.detect_language(file_path)
        lines = content.split('\n')
        
        metrics = {
            'total_lines': len(lines),
            'comment_chars': 0,
            'code_chars': 0,
            'empty_lines': 0,
            'import_lines': 0,
            'boilerplate_lines': 0,
            'useful_code_percent': 0
        }
        
        comment_patterns = {
            'single_line': [r'//.*$', r'#.*$', r'--.*$', r';.*$', r'%.*$', r"\'.*$"],
            'multi_line': [(r'/\*', r'\*/'), (r'<!--', r'-->'), (r'"""', r'"""'), (r"'''", r"'''")]
        }
        
        in_multiline_comment = False
        current_multiline_type = None
        useful_code_chars = 0
        
        for line in lines:
            line_stripped = line.strip()
            
            if not line_stripped:
                metrics['empty_lines'] += 1
                continue
            
            if in_multiline_comment:
                metrics['comment_chars'] += len(line)
                end_pattern = current_multiline_type[1]
                if re.search(end_pattern, line):
                    in_multiline_comment = False
                continue
            
            multiline_found = False
            for start_pattern, end_pattern in comment_patterns['multi_line']:
                if re.search(start_pattern, line):
                    metrics['comment_chars'] += len(line)
                    multiline_found = True
                    if not re.search(end_pattern, line):
                        in_multiline_comment = True
                        current_multiline_type = (start_pattern, end_pattern)
                    break
            
            if multiline_found:
                continue
            
            line_without_comments = line
            has_comment = False
            
            for pattern in comment_patterns['single_line']:
                match = re.search(pattern, line)
                if match:
                    comment_text = match.group(0)
                    metrics['comment_chars'] += len(comment_text)
                    line_without_comments = re.sub(pattern, '', line)
                    has_comment = True
                    break
            
            line_clean = line_without_comments.strip()
            
            if not line_clean and has_comment:
                continue
            
            if not line_clean:
                continue
            
            is_import = any(re.match(pattern, line_clean) for pattern in self.boilerplate_patterns['import_patterns'])
            if is_import:
                metrics['import_lines'] += 1
                metrics['boilerplate_lines'] += 1
                continue
            
            is_boilerplate = any(re.match(pattern, line_clean) for pattern in self.boilerplate_patterns['boilerplate_patterns'])
            if is_boilerplate:
                metrics['boilerplate_lines'] += 1
                continue
            
            useful_code_chars += len(line_clean)
            metrics['code_chars'] += len(line_clean)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–µ–∑–Ω–æ–≥–æ –∫–æ–¥–∞
        if metrics['total_lines'] > 0:
            useful_lines = metrics['total_lines'] - metrics['empty_lines'] - metrics['import_lines'] - metrics['boilerplate_lines']
            metrics['useful_code_percent'] = (useful_lines / metrics['total_lines']) * 100
        
        return metrics

    def find_todo_comments(self, content, file_path):
        """–ù–∞—Ö–æ–¥–∏—Ç TODO –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –º–µ—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è"""
        if not self.config.get('scanTodos', True):
            return []
        
        language = self.detect_language(file_path)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        comment_patterns = {
            'java': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'javascript': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'python': [r'#[^\n]*', r'""".*?"""', r"'''.*?'''"],
            'cpp': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'c': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'html': [r'<!--.*?-->'],
            'css': [r'/\*[\s\S]*?\*/'],
            'php': [r'//[^\n]*', r'#[^\n]*', r'/\*[\s\S]*?\*/'],
            'ruby': [r'#[^\n]*', r'=begin[\s\S]*?=end'],
            'go': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'rust': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'swift': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'kotlin': [r'//[^\n]*', r'/\*[\s\S]*?\*/']
        }
        
        patterns = comment_patterns.get(language, [])
        todos = []
        found_signatures = set()  # –î–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
        for pattern in patterns:
            try:
                matches = re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE)
                for match in matches:
                    comment_text = match.group().strip()
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                    if len(comment_text) < self.config.get('minCommentLength', 20):
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
                    line_num = content[:match.start()].count('\n') + 1
                    comment_signature = f"{file_path}:{line_num}:{comment_text[:50]}"
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if comment_signature in found_signatures:
                        continue
                    found_signatures.add(comment_signature)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ TODO –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                    has_todo = False
                    for keyword in self.todo_patterns['todo_keywords']:
                        if re.search(keyword, comment_text, re.IGNORECASE):
                            has_todo = True
                            break
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ñ—Ä–∞–∑—ã —É–ª—É—á—à–µ–Ω–∏—è
                    has_improvement = False
                    for phrase in self.todo_patterns['improvement_phrases']:
                        if re.search(phrase, comment_text, re.IGNORECASE):
                            has_improvement = True
                            break
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
                    if has_todo and has_improvement:
                        todo_type = 'TODO_IMPROVEMENT'
                    elif has_todo:
                        todo_type = 'TODO'
                    elif has_improvement:
                        todo_type = 'IMPROVEMENT'
                    else:
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è TODO
                    todo_key = f"{file_path}:{line_num}:{comment_text}"
                    if todo_key in self.stats['unique_todos']:
                        continue
                    self.stats['unique_todos'].add(todo_key)
                    
                    todos.append({
                        'type': todo_type,
                        'text': comment_text,
                        'line': line_num,
                        'language': language,
                        'file': file_path
                    })
                    
            except Exception as e:
                if self.config.get('verbose'):
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ TODO –≤ {file_path}: {e}")
        
        return todos

    def clean_comments(self, content, file_path):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –∫—Ä–æ–º–µ TODO –∏ –≤–∞–∂–Ω—ã—Ö –ø–æ–º–µ—Ç–æ–∫"""
        language = self.detect_language(file_path)
        
        comment_patterns = {
            'java': [
                (r'//(?![^\n]*\b(TODO|FIXME|XXX|HACK|NOTE|OPTIMIZE|REVIEW|IMPROVE|–Ω—É–∂–Ω–æ|—Å–ª–µ–¥—É–µ—Ç|–Ω–∞–¥–æ|—Ç—Ä–µ–±—É–µ—Ç—Å—è|—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å|–¥–æ–¥–µ–ª–∞—Ç—å|—Å–¥–µ–ª–∞—Ç—å|–≤—Ä–µ–º–µ–Ω–Ω|–∫–æ—Å—Ç—ã–ª—å|–∑–∞–≥–ª—É—à–∫–∞|—É–ø—Ä–æ—â[–µ—ë]–Ω–Ω|–≤ –ø—Ä–æ–¥–∞–∫—à–Ω|–ª—É—á—à–µ —Å–¥–µ–ª–∞—Ç—å|–Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω|–º–µ–¥–ª–µ–Ω–Ω–æ|–ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å|—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥|–Ω–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω|–¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É|–Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ|—É–ø—Ä–æ—Å—Ç–∏—Ç—å|—É–ª—É—á—à–∏—Ç—å)\b)[^\n]*', ''),
                (r'/\*(?![\s\S]*\b(TODO|FIXME|XXX|HACK|NOTE|OPTIMIZE|REVIEW|IMPROVE|–Ω—É–∂–Ω–æ|—Å–ª–µ–¥—É–µ—Ç|–Ω–∞–¥–æ|—Ç—Ä–µ–±—É–µ—Ç—Å—è|—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å|–¥–æ–¥–µ–ª–∞—Ç—å|—Å–¥–µ–ª–∞—Ç—å|–≤—Ä–µ–º–µ–Ω–Ω|–∫–æ—Å—Ç—ã–ª—å|–∑–∞–≥–ª—É—à–∫–∞|—É–ø—Ä–æ—â[–µ—ë]–Ω–Ω|–≤ –ø—Ä–æ–¥–∞–∫—à–Ω|–ª—É—á—à–µ —Å–¥–µ–ª–∞—Ç—å|–Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω|–º–µ–¥–ª–µ–Ω–Ω–æ|–ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å|—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥|–Ω–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω|–¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É|–Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ|—É–ø—Ä–æ—Å—Ç–∏—Ç—å|—É–ª—É—á—à–∏—Ç—å)\b)[\s\S]*?\*/', ''),
            ],
            'python': [
                (r'#(?![^\n]*\b(TODO|FIXME|XXX|HACK|NOTE|OPTIMIZE|REVIEW|IMPROVE|–Ω—É–∂–Ω–æ|—Å–ª–µ–¥—É–µ—Ç|–Ω–∞–¥–æ|—Ç—Ä–µ–±—É–µ—Ç—Å—è|—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å|–¥–æ–¥–µ–ª–∞—Ç—å|—Å–¥–µ–ª–∞—Ç—å|–≤—Ä–µ–º–µ–Ω–Ω|–∫–æ—Å—Ç—ã–ª—å|–∑–∞–≥–ª—É—à–∫–∞|—É–ø—Ä–æ—â[–µ—ë]–Ω–Ω|–≤ –ø—Ä–æ–¥–∞–∫—à–Ω|–ª—É—á—à–µ —Å–¥–µ–ª–∞—Ç—å|–Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω|–º–µ–¥–ª–µ–Ω–Ω–æ|–ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å|—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥|–Ω–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω|–¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É|–Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ|—É–ø—Ä–æ—Å—Ç–∏—Ç—å|—É–ª—É—á—à–∏—Ç—å)\b)[^\n]*', ''),
            ]
        }
        
        patterns = comment_patterns.get(language, [])
        original_length = len(content)
        cleaned_content = content
        
        for pattern, replacement in patterns:
            try:
                cleaned_content = re.sub(pattern, replacement, cleaned_content)
            except Exception as e:
                if self.config.get('verbose'):
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ {file_path}: {e}")
        
        removed_chars = original_length - len(cleaned_content)
        removed_lines = cleaned_content.count('\n') - content.count('\n')
        
        self.stats['clean_removed_chars'] += removed_chars
        self.stats['clean_removed_lines'] += abs(removed_lines)
        
        return cleaned_content

    def normalize_line_endings(self, content):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫"""
        line_endings = self.config.get('lineEndings', 'preserve')
        
        if line_endings == 'unix':
            return content.replace('\r\n', '\n').replace('\r', '\n')
        elif line_endings == 'windows':
            return content.replace('\r\n', '\n').replace('\r', '\n').replace('\n', '\r\n')
        else:
            return content
    
    def write_header(self, output_file, part_info=None):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª"""
        if self.config.get('addHeader'):
            part_suffix = f" | Part {part_info['current']} of {part_info['total']}" if part_info else ""
            size_mb = self.stats['total_size_bytes'] / (1024 * 1024)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–∏—Ñ—Ä—ã
            todo_count = sum(1 for todo in self.stats['all_todos'] if 'TODO' in todo['type'])
            improvement_count = sum(1 for todo in self.stats['all_todos'] if 'IMPROVEMENT' in todo['type'])
            
            todo_info = ""
            if self.config.get('showTodosInStats', True):
                todo_info = f"""
 * TODO comments: {todo_count}
 * Improvement suggestions: {improvement_count}"""
            
            header = f"""/*
 * Generated by CodeAnalyzer
 * Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
 * Total files: {self.stats['processed_files']}
 * Total lines: {self.stats['total_lines']:,}
 * Total size: {self.stats['total_size_bytes']:,} bytes ({size_mb:.2f} MB)
 * Estimated tokens: {self.stats['total_tokens']:,}{todo_info}{part_suffix}
 */

"""
            output_file.write(header)
    
    def get_useful_code_percentage(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–µ–∑–Ω–æ–≥–æ –∫–æ–¥–∞"""
        if self.stats['total_lines'] == 0:
            return 0
        return (self.stats['useful_code_lines'] / self.stats['total_lines']) * 100
    
    def estimate_tokens(self, text):
        """–ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        words = len(text.split())
        chars = len(text)
        token_estimate = max(words * 0.75, chars / 4)
        return int(token_estimate)
    
    def split_into_parts(self, content):
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–∞ —á–∞—Å—Ç–∏, —Å–æ—Ö—Ä–∞–Ω—è—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–æ–≤"""
        split_into_parts = self.config.get('splitIntoParts', 1)
        max_tokens_per_part = self.config.get('maxTokensPerPart', 0)
        
        if self.config.get('verbose'):
            print(f"DEBUG: splitIntoParts = {split_into_parts}, maxTokensPerPart = {max_tokens_per_part}")
        
        if split_into_parts <= 1 and max_tokens_per_part <= 0:
            if self.config.get('verbose'):
                print("DEBUG: No splitting needed")
            return [content]
        
        separator_template = self.config.get('separator', '/* %path% */')
        separator_pattern = re.escape(separator_template)
        separator_pattern = separator_pattern.replace(re.escape('%path%'), '.*?')
        
        matches = list(re.finditer(separator_pattern, content))
        
        if self.config.get('verbose'):
            print(f"DEBUG: Found {len(matches)} separator matches")
        
        if not matches:
            if self.config.get('verbose'):
                print("DEBUG: No separators found, cannot split")
            return [content]
        
        file_sections = []
        last_end = 0
        
        for i, match in enumerate(matches):
            if i > 0:
                file_end = matches[i-1].end()
                file_content = content[last_end:file_end]
                file_sections.append(file_content)
                last_end = file_end
            
            if i == len(matches) - 1:
                file_content = content[last_end:]
                file_sections.append(file_content)
        
        if self.config.get('verbose'):
            print(f"DEBUG: Split into {len(file_sections)} file sections")
        
        if split_into_parts > 1:
            files_per_part = max(1, len(file_sections) // split_into_parts)
            parts = []
            current_part = []
            
            if self.config.get('verbose'):
                print(f"DEBUG: Splitting into {split_into_parts} parts, approx {files_per_part} files per part")
            
            for i, file_content in enumerate(file_sections):
                current_part.append(file_content)
                
                if (len(current_part) >= files_per_part and 
                    len(parts) < split_into_parts - 1):
                    parts.append(''.join(current_part))
                    current_part = []
                    if self.config.get('verbose'):
                        print(f"DEBUG: Created part {len(parts)} with {i+1} files")
            
            if current_part:
                parts.append(''.join(current_part))
                if self.config.get('verbose'):
                    print(f"DEBUG: Created final part {len(parts)} with {len(current_part)} files")
            
            if self.config.get('verbose'):
                print(f"DEBUG: Total parts created: {len(parts)}")
            return parts
        
        elif max_tokens_per_part > 0:
            parts = []
            current_part = []
            current_tokens = 0
            
            for file_content in file_sections:
                file_tokens = self.estimate_tokens(file_content)
                
                if file_tokens > max_tokens_per_part:
                    print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Ñ–∞–π–ª –ø—Ä–µ–≤—ã—à–∞–µ—Ç maxTokensPerPart ({file_tokens} > {max_tokens_per_part})")
                    if current_part:
                        parts.append(''.join(current_part))
                    parts.append(file_content)
                    current_part = []
                    current_tokens = 0
                elif current_tokens + file_tokens > max_tokens_per_part and current_part:
                    parts.append(''.join(current_part))
                    current_part = [file_content]
                    current_tokens = file_tokens
                else:
                    current_part.append(file_content)
                    current_tokens += file_tokens
            
            if current_part:
                parts.append(''.join(current_part))
            
            return parts
        
        return [content]
    
    def get_file_stats(self, file_path, content):
        """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ñ–∞–π–ª—É"""
        metrics = self.analyze_code_metrics(content, file_path)
        
        # –í–†–ï–ú–ï–ù–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è TODO
        important_comments = self.find_todo_comments(content, file_path)
        
        # –í–†–ï–ú–ï–ù–ù–û –æ—Ç–∫–ª—é—á–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –Ω–µ-Java —Ñ–∞–π–ª–æ–≤
        if self.detect_language(file_path) == 'java':
            try:
                # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º TODO
                extended_comments = self.find_important_comments(content, file_path)
                important_comments.extend(extended_comments)
            except Exception as e:
                if self.config.get('verbose'):
                    print(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {e}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        complexity_metrics = {}
        hotspots = []
        dependencies = {}
        duplicates = []
        style_issues = []
        test_coverage = {}
        
        if self.detect_language(file_path) == 'java':
            complexity_metrics = self.analyze_java_complexity(content, file_path)
            hotspots = self.find_code_hotspots(content, file_path)
            dependencies = self.analyze_dependencies(content, file_path)
            duplicates = self.find_duplicate_code(content, file_path)
            style_issues = self.check_code_style(content, file_path)
            test_coverage = self.analyze_test_coverage(file_path)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º defaults –¥–ª—è complexity_metrics
        complexity_metrics.setdefault('method_count', 0)
        complexity_metrics.setdefault('class_count', 0)
        complexity_metrics.setdefault('avg_method_lines', 0)
        complexity_metrics.setdefault('max_method_lines', 0)
        complexity_metrics.setdefault('complex_methods', [])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
        for comment in important_comments:
            comment['file'] = os.path.relpath(file_path, '.')
            self.stats['all_todos'].append(comment)
        
        stats = {
            'lines': metrics['total_lines'],
            'size': os.path.getsize(file_path),
            'tokens': self.estimate_tokens(content),
            'encoding': self.config.get('encoding', 'utf-8'),
            'important_comments': important_comments,  # –ó–ê–ú–ï–ù–ò–õ–ò 'todos'
            'metrics': metrics,
            'complexity': complexity_metrics,
            'hotspots': hotspots,
            'dependencies': dependencies,
            'duplicates': duplicates,
            'style_issues': style_issues,
            'test_coverage': test_coverage,
            'language': self.detect_language(file_path)
        }
        return stats
    
    def print_file_info(self, relative_path, stats):
        """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω verbose —Ä–µ–∂–∏–º"""
        if self.config.get('verbose') and self.config.get('showFileStats'):
            todo_info = ""
            if stats['important_comments']:  # –ó–ê–ú–ï–ù–ò–õ–ò 'todos'
                todo_count = len(stats['important_comments'])  # –ó–ê–ú–ï–ù–ò–õ–ò 'todos'
                todo_info = f" - ‚ö†Ô∏è {todo_count} –≤–∞–∂–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"
            
            complexity_info = ""
            if stats['complexity'] and stats['complexity'].get('complex_methods'):
                complex_count = len(stats['complexity']['complex_methods'])
                complexity_info = f" - üî• {complex_count} —Å–ª–æ–∂–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤"
            
            print(f"‚úì {relative_path} - {stats['metrics']['useful_code_percent']:.1f}% –ø–æ–ª–µ–∑–Ω–æ–≥–æ –∫–æ–¥–∞{todo_info}{complexity_info}")
           
    def print_quality_metrics(self):
        """–í—ã–≤–æ–¥–∏—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞"""
        if not self.config.get('analyzeCodeQuality', True):
            return
        
        print("\n" + "üîß –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê –ö–û–î–ê:")
        print("-" * 50)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º file_metrics –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        file_metrics = getattr(self.stats, 'file_metrics', {})
        
        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        total_methods = 0
        total_classes = 0
        avg_method_length_sum = 0
        file_metrics_count = 0
        
        for metrics in file_metrics.values():
            total_methods += metrics.get('method_count', 0)
            total_classes += metrics.get('class_count', 0)
            if metrics.get('avg_method_lines', 0) > 0:
                avg_method_length_sum += metrics.get('avg_method_lines', 0)
                file_metrics_count += 1
        
        if total_methods > 0:
            avg_method_length = avg_method_length_sum / file_metrics_count if file_metrics_count > 0 else 0
            print(f"üìä –ú–µ—Ç–æ–¥–æ–≤: {total_methods}")
            print(f"üìä –ö–ª–∞—Å—Å–æ–≤: {total_classes}")
            if avg_method_length > 0:
                print(f"üìä –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –º–µ—Ç–æ–¥–∞: {avg_method_length:.1f} —Å—Ç—Ä–æ–∫")
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥
        tech_debt_score = self.calculate_tech_debt_score()
        debt_level = "üü¢ –ù–∏–∑–∫–∏–π" if tech_debt_score < 20 else "üü° –°—Ä–µ–¥–Ω–∏–π" if tech_debt_score < 50 else "üî¥ –í—ã—Å–æ–∫–∏–π"
        print(f"üìä –û—Ü–µ–Ω–∫–∞ —Ç–µ—Ö–¥–æ–ª–≥–∞: {tech_debt_score}/100 ({debt_level})")
	    
        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ñ–∞–π–ª—ã
        complex_files = [f for f, metrics in file_metrics.items() 
                        if metrics.get('complex_methods')]
        if complex_files:
            print(f"üî• –§–∞–π–ª–æ–≤ —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏: {len(complex_files)}")
        
        # –ì–æ—Ä—è—á–∏–µ —Ç–æ—á–∫–∏
        hotspots_dict = getattr(self.stats, 'hotspots', {})
        total_hotspots = sum(len(hotspots) for hotspots in hotspots_dict.values())
        if total_hotspots > 0:
	        print(f"üö® –ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–µ—Å—Ç: {total_hotspots}")

        total_duplicates = sum(len(stats.get('duplicates', [])) 
                          for stats in self.stats.get('file_stats', {}).values())
        total_style_issues = sum(len(stats.get('style_issues', [])) 
                           for stats in self.stats.get('file_stats', {}).values())
                           
        if total_duplicates > 0:
            print(f"üîÅ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {total_duplicates}")
    
        if total_style_issues > 0:
	        print(f"üé® –ü—Ä–æ–±–ª–µ–º —Å—Ç–∏–ª—è: {total_style_issues}")
	    
	    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥
        self.export_complex_methods()
    
    def export_todos_to_file(self):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≤—Å–µ TODO –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª"""
        if not self.stats['all_todos']:
            return
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º TODO –ø–æ —Ñ–∞–π–ª–∞–º –∏ —Å—Ç—Ä–æ–∫–∞–º
        todos_by_file = {}
        for todo in self.stats['all_todos']:
            file_path = todo['file']
            if file_path not in todos_by_file:
                todos_by_file[file_path] = []
            todos_by_file[file_path].append(todo)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –∏ TODO –≤–Ω—É—Ç—Ä–∏ —Ñ–∞–π–ª–æ–≤
        for file_path in todos_by_file:
            todos_by_file[file_path].sort(key=lambda x: x['line'])
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ñ–∞–π–ª
        with open('important_comments.txt', 'w', encoding='utf-8') as f:
            f.write("üí° –í–ê–ñ–ù–´–ï –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ò –ò TODO\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(self.stats['all_todos'])} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n")
            f.write(f"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            for file_path, file_todos in sorted(todos_by_file.items()):
                f.write(f"\nüìÅ {file_path}\n")
                f.write("‚îÄ" * 50 + "\n")
                
                for todo in file_todos:
                    if todo['type'] == 'TODO':
                        icon = "üî∏ TODO"
                    elif todo['type'] == 'IMPROVEMENT':
                        icon = "üí° IMPROVEMENT"
                    else:
                        icon = "üî∏üí° TODO+IMPROVEMENT"
                    
                    f.write(f"\n{icon} (—Å—Ç—Ä–æ–∫–∞ {todo['line']}):\n")
                    f.write(f"   {todo['text']}\n")
    
    def print_todo_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º TODO –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º"""
        if not self.stats['all_todos']:
            return
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        todo_count = sum(1 for todo in self.stats['all_todos'] if 'TODO' in todo['type'])
        improvement_count = sum(1 for todo in self.stats['all_todos'] if 'IMPROVEMENT' in todo['type'])
        
        print("\n" + "üîç –ù–ê–ô–î–ï–ù–ù–´–ï TODO –ò –ú–ï–°–¢–ê –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–ô:")
        print("-" * 50)
        print(f"üìã TODO –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {todo_count}")
        print(f"üí° –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é: {improvement_count}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–∞–π–ª
        if self.config.get('exportTodos', True):
            self.export_todos_to_file()
            print(f"üìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: important_comments.txt")
        
        # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫–∏–π —Å–ø–∏—Å–æ–∫
        print(f"\nüìù –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–µ—Å—Ç–∞:")
        print("-" * 30)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª–∞–º
        todos_by_file = {}
        for todo in self.stats['all_todos']:
            file_path = todo['file']
            if file_path not in todos_by_file:
                todos_by_file[file_path] = []
            todos_by_file[file_path].append(todo)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ 2 –ø—Ä–∏–º–µ—Ä–∞ –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        for file_path, file_todos in list(todos_by_file.items())[:6]:
            print(f"\nüìÅ {file_path}:")
            for todo in file_todos[:2]:
                icon = "üî∏" if 'TODO' in todo['type'] else "üí°"
                preview = todo['text'][:80] + "..." if len(todo['text']) > 80 else todo['text']
                print(f"  {icon} –°—Ç—Ä–æ–∫–∞ {todo['line']}: {preview}")
            
            if len(file_todos) > 2:
                print(f"  ... –∏ –µ—â—ë {len(file_todos) - 2}")
    
    def analyze_project(self, root_dir='.'):
	    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–µ–∫—Ç –±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
	    print("üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞...")
	    
	    # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ TODO
	    self.stats['all_todos'] = []
	    self.stats['unique_todos'] = set()
	    self.stats['file_metrics'] = {}  # ‚Üê –î–û–ë–ê–í–ò–¢–¨ –≠–¢–û
	    self.stats['hotspots'] = {}      # ‚Üê –î–û–ë–ê–í–ò–¢–¨ –≠–¢–û
	    
	    for root, dirs, files in os.walk(root_dir):
	        dirs[:] = [d for d in dirs if self.should_include_file(
	            os.path.join(root, d), 
	            os.path.relpath(os.path.join(root, d), root_dir)
	        )]
	        
	        for file in files:
	            file_path = os.path.join(root, file)
	            relative_path = os.path.relpath(file_path, root_dir)
	            
	            self.stats['total_files'] += 1
	            
	            if not self.should_include_file(file_path, relative_path):
	                self.stats['excluded_files'] += 1
	                continue
	            
	            if self.config.get('skipBinary', True) and self.is_binary_file(file_path):
	                self.stats['binary_files'] += 1
	                continue
	            
	            try:
	                with open(file_path, 'r', encoding=self.config.get('encoding', 'utf-8')) as input_file:
	                    content = input_file.read()
	                
	                stats = self.get_file_stats(file_path, content)
	                
	                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ hotspots
	                if self.config.get('analyzeComplexity', True):
	                    self.stats['file_metrics'][relative_path] = stats['complexity']
	                    self.stats['hotspots'][relative_path] = stats['hotspots']
	                
	                # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
	                self.stats['total_lines'] += stats['metrics']['total_lines']
	                self.stats['total_size_bytes'] += stats['size']
	                self.stats['total_tokens'] += stats['tokens']
	                self.stats['useful_code_percent'] = stats['metrics']['useful_code_percent']  # ‚Üê –û–ë–ù–û–í–ò–¢–¨
	                self.stats['empty_lines'] += stats['metrics']['empty_lines']
	                
	                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º
	                lang = stats['language']
	                if lang not in self.stats['files_by_language']:
	                    self.stats['files_by_language'][lang] = 0
	                self.stats['files_by_language'][lang] += 1
	                
	                self.stats['processed_files'] += 1
	                self.print_file_info(relative_path, stats)
	                    
	            except Exception as e:
	                self.stats['error_files'] += 1
	                if self.config.get('verbose'):
	                    print(f"‚úó –û—à–∏–±–∫–∞: {relative_path} - {e}")
    
    def compile_project(self, root_dir='.'):
        """–ö–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç –ø—Ä–æ–µ–∫—Ç –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª (–∞–Ω–∞–ª–∏–∑ + –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ)"""
        self.analyze_project(root_dir)
        self.split_project(root_dir)
    
    def split_project(self, root_dir='.'):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞ –≤ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤"""
        output_filename = self.config.get('filename')
        separator_template = self.config.get('separator')
        encoding = self.config.get('encoding', 'utf-8')
        
        file_contents = []
        
        print("üì¶ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
        
        for root, dirs, files in os.walk(root_dir):
            dirs[:] = [d for d in dirs if self.should_include_file(
                os.path.join(root, d), 
                os.path.relpath(os.path.join(root, d), root_dir)
            )]
            
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, root_dir)
                
                if not self.should_include_file(file_path, relative_path):
                    continue
                
                if self.config.get('skipBinary', True) and self.is_binary_file(file_path):
                    continue
                
                separator = separator_template.replace('%path%', relative_path)
                
                try:
                    with open(file_path, 'r', encoding=encoding) as input_file:
                        content = input_file.read()
                        content = self.normalize_line_endings(content)
                    
                    file_stats = self.get_file_stats(file_path, content)
                    todo_header = ""
                    if file_stats['important_comments']:  # –ó–ê–ú–ï–ù–ò–õ–ò 'todos'
                        todo_count = len(file_stats['important_comments'])  # –ó–ê–ú–ï–ù–ò–õ–ò 'todos'
                        todo_header = f"\n/* ‚ö†Ô∏è Found {todo_count} important comment(s) */\n"
                    
                    file_content = separator + todo_header + '\n\n' + content
                    if content and not content.endswith('\n'):
                        file_content += '\n'
                    
                    file_contents.append(file_content)
                        
                except Exception as e:
                    error_msg = f"{separator}\n\n/* ERROR reading {relative_path}: {str(e)} */\n"
                    file_contents.append(error_msg)
        
        if not file_contents:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
            return
        
        full_content = '\n\n'.join(file_contents)
        parts = self.split_into_parts(full_content)
        
        if len(parts) == 1:
            with open(output_filename, 'w', encoding=encoding) as output_file:
                self.write_header(output_file)
                output_file.write(parts[0])
            print(f"üìÑ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {output_filename}")
        else:
            base_name = Path(output_filename).stem
            extension = Path(output_filename).suffix
            
            for i, part_content in enumerate(parts, 1):
                part_filename = f"{base_name}_part{i}{extension}"
                with open(part_filename, 'w', encoding=encoding) as output_file:
                    part_info = {'current': i, 'total': len(parts)}
                    self.write_header(output_file, part_info)
                    output_file.write(part_content)
                
                part_size = len(part_content)
                part_lines = len(part_content.splitlines())
                print(f"üìÑ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {part_filename} ({part_size:,} —Å–∏–º–≤–æ–ª–æ–≤, {part_lines:,} —Å—Ç—Ä–æ–∫)")
        
        if self.config.get('scanTodos', True):
            self.print_todo_summary()
    
    def clean_project(self, root_dir='.'):
        """–û—á–∏—â–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è TODO"""
        print("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤...")
        
        output_filename = self.config.get('filename', 'cleaned_project.txt')
        separator_template = self.config.get('separator')
        encoding = self.config.get('encoding', 'utf-8')
        
        file_contents = []
        
        for root, dirs, files in os.walk(root_dir):
            dirs[:] = [d for d in dirs if self.should_include_file(
                os.path.join(root, d), 
                os.path.relpath(os.path.join(root, d), root_dir)
            )]
            
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, root_dir)
                
                if not self.should_include_file(file_path, relative_path):
                    continue
                
                if self.config.get('skipBinary', True) and self.is_binary_file(file_path):
                    continue
                
                separator = separator_template.replace('%path%', relative_path)
                
                try:
                    with open(file_path, 'r', encoding=encoding) as input_file:
                        content = input_file.read()
                    
                    cleaned_content = self.clean_comments(content, file_path)
                    cleaned_content = self.normalize_line_endings(cleaned_content)
                    
                    file_content = separator + '\n\n' + cleaned_content
                    if cleaned_content and not cleaned_content.endswith('\n'):
                        file_content += '\n'
                    
                    file_contents.append(file_content)
                    
                    if self.config.get('verbose'):
                        original_size = len(content)
                        cleaned_size = len(cleaned_content)
                        reduction = ((original_size - cleaned_size) / original_size) * 100
                        print(f"üßπ {relative_path} - –æ—á–∏—â–µ–Ω–æ {reduction:.1f}%")
                        
                except Exception as e:
                    error_msg = f"{separator}\n\n/* ERROR cleaning {relative_path}: {str(e)} */\n"
                    file_contents.append(error_msg)
        
        if not file_contents:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
            return
        
        full_content = '\n\n'.join(file_contents)
        
        with open(output_filename, 'w', encoding=encoding) as output_file:
            self.write_header(output_file)
            output_file.write(full_content)
        
        print(f"üßπ –°–æ–∑–¥–∞–Ω –æ—á–∏—â–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {output_filename}")
        print(f"üßπ –£–¥–∞–ª–µ–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {self.stats['clean_removed_chars']} —Å–∏–º–≤–æ–ª–æ–≤")
    
    def print_detailed_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–Ω–∞–ª–∏–∑–∞"""
        print("\n" + "="*70)
        print("üìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–ï–ö–¢–ê")
        print("="*70)
        
        print(f"?? –§–∞–π–ª—ã:")
        print(f"  –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {self.stats['total_files']:,}")
        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['processed_files']:,} ({self.get_processed_percentage():.1f}%)")
        print(f"  –ò—Å–∫–ª—é—á–µ–Ω–æ: {self.stats['excluded_files']:,}")
        print(f"  –ë–∏–Ω–∞—Ä–Ω—ã—Ö: {self.stats['binary_files']:,}")
        print(f"  –° –æ—à–∏–±–∫–∞–º–∏: {self.stats['error_files']:,}")
        
        print(f"\nüìè –†–∞–∑–º–µ—Ä—ã:")
        size_kb = self.stats['total_size_bytes'] / 1024
        size_mb = self.stats['total_size_bytes'] / (1024 * 1024)
        print(f"  –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {self.stats['total_size_bytes']:,} –±–∞–π—Ç ({size_kb:,.1f} KB / {size_mb:.2f} MB)")
        print(f"  –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {self.stats['total_lines']:,}")
        print(f"  –¢–æ–∫–µ–Ω–æ–≤: ~{self.stats['total_tokens']:,}")
        
        print(f"\nüíª –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–∞:")
        print(f"  –ü–æ–ª–µ–∑–Ω–æ–≥–æ –∫–æ–¥–∞: {self.stats['useful_code_percent']:.1f}%")
        print(f"  –ü—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {self.get_empty_line_percentage():.1f}%")
        
        print(f"\n‚ö†Ô∏è  –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–µ—Å—Ç–∞:")
        todo_count = sum(1 for todo in self.stats['all_todos'] if 'TODO' in todo['type'])
        improvement_count = sum(1 for todo in self.stats['all_todos'] if 'IMPROVEMENT' in todo['type'])
        print(f"  TODO –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {todo_count}")
        print(f"  –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é: {improvement_count}")
        
        if self.stats['files_by_language']:
            print(f"\nüåê –Ø–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:")
            for lang, count in sorted(self.stats['files_by_language'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / self.stats['processed_files']) * 100
                print(f"  {lang}: {count} —Ñ–∞–π–ª–æ–≤ ({percentage:.1f}%)")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        self.print_quality_metrics()
        
        print("="*70)
        
    def get_processed_percentage(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        if self.stats['total_files'] == 0:
            return 0
        return (self.stats['processed_files'] / self.stats['total_files']) * 100
    
    def get_code_percentage(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç –∫–æ–¥–∞"""
        if self.stats['total_lines'] == 0:
            return 0
        return (self.stats['code_lines'] / self.stats['total_lines']) * 100
    
    def get_empty_line_percentage(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫"""
        if self.stats['total_lines'] == 0:
            return 0
        return (self.stats['empty_lines'] / self.stats['total_lines']) * 100
        
    def analyze_dependencies(self, content, file_path):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ)"""
        dependencies = {
            'imports': [],
            'internal_deps': [],
            'external_deps': []
        }
        
        language = self.detect_language(file_path)
        
        try:
            if language == 'java':
                # –ò–º–ø–æ—Ä—Ç—ã Java
                import_pattern = r'import\s+([\w\.]+)\s*;'
                imports = re.findall(import_pattern, content)
                dependencies['imports'] = imports
                
                # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–¥—Ä—É–≥–∏–µ –∫–ª–∞—Å—Å—ã –ø—Ä–æ–µ–∫—Ç–∞)
                class_ref_pattern = r'new\s+(\w+)\s*\('
                class_refs = re.findall(class_ref_pattern, content)
                dependencies['internal_deps'] = list(set(class_refs))
                
            elif language == 'python':
                # –ê–Ω–∞–ª–∏–∑ AST Python –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
                try:
                    tree = ast.parse(content)
                    for node in ast.walk(tree):
                        if isinstance(node, ast.Import):
                            for alias in node.names:
                                dependencies['imports'].append(alias.name)
                        elif isinstance(node, ast.ImportFrom):
                            if node.module:
                                dependencies['imports'].append(node.module)
                except:
                    # Fallback: regex –¥–ª—è Python
                    import_pattern = r'^(?:from\s+(\w+)\s+import|import\s+([\w\s,]+))'
                    imports = re.findall(import_pattern, content, re.MULTILINE)
                    for imp in imports:
                        dependencies['imports'].extend([x for x in imp if x])
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ/–≤–Ω–µ—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            dependencies['external_deps'] = [dep for dep in dependencies['imports'] 
                                           if not any(proj_keyword in dep.lower() 
                                           for proj_keyword in ['com.kaleert', 'xbot'])]
            
        except Exception as e:
            if self.config.get('verbose'):
                print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π {file_path}: {e}")
        
        return dependencies
	
    def find_duplicate_code(self, content, file_path):
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –¥—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è –∫–æ–¥ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ)"""
        duplicates = []
        language = self.detect_language(file_path)
        
        try:
            lines = content.split('\n')
            code_blocks = {}
            
            # –°–æ–±–∏—Ä–∞–µ–º –±–ª–æ–∫–∏ –∫–æ–¥–∞ (–º–µ–∂–¥—É { –∏ } –¥–ª—è Java/C-like —è–∑—ã–∫–æ–≤)
            if language in ['java', 'javascript', 'cpp', 'c']:
                brace_count = 0
                current_block = []
                block_start = 0
                
                for i, line in enumerate(lines):
                    stripped = line.strip()
                    
                    if '{' in stripped:
                        if brace_count == 0:
                            block_start = i
                        brace_count += stripped.count('{')
                        current_block.append(stripped)
                    
                    elif '}' in stripped:
                        brace_count -= stripped.count('}')
                        current_block.append(stripped)
                        
                        if brace_count == 0 and len(current_block) > 3:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–ª–æ–∫ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –±–ª–æ–∫–∏)
                            block_content = '\n'.join([l for l in current_block if l])
                            if len(block_content) > 50:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –±–ª–æ–∫–∏
                                key = hash(block_content)
                                if key in code_blocks:
                                    duplicates.append({
                                        'block': block_content,
                                        'locations': [block_start, i],
                                        'duplicate_of': code_blocks[key]['locations'][0]
                                    })
                                else:
                                    code_blocks[key] = {
                                        'content': block_content,
                                        'locations': [block_start, i]
                                    }
                            current_block = []
            
            # –î–ª—è Python –∏—â–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—É–Ω–∫—Ü–∏–∏
            elif language == 'python':
                function_pattern = r'def\s+(\w+)\s*\([^)]*\):'
                functions = list(re.finditer(function_pattern, content))
                
                for i, func1 in enumerate(functions):
                    for func2 in functions[i+1:]:
                        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–∏–≥–Ω–∞—Ç—É—Ä—ã —Ñ—É–Ω–∫—Ü–∏–π
                        if func1.group(1) == func2.group(1):
                            duplicates.append({
                                'type': 'duplicate_function',
                                'name': func1.group(1),
                                'locations': [func1.start(), func2.start()]
                            })
            
        except Exception as e:
            if self.config.get('verbose'):
                print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ {file_path}: {e}")
        
        return duplicates
	
    def check_code_style(self, content, file_path):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç code style (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ)"""
        style_issues = []
        language = self.detect_language(file_path)
        
        lines = content.split('\n')
        
        for i, line in enumerate(lines, 1):
            stripped = line.rstrip()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ trailing whitespace
            if line != stripped and stripped:
                style_issues.append({
                    'line': i,
                    'issue': 'Trailing whitespace',
                    'severity': 'low'
                })
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Å—Ç—Ä–æ–∫–∏
            if len(line) > 120:
                style_issues.append({
                    'line': i, 
                    'issue': f'Line too long ({len(line)} characters)',
                    'severity': 'medium'
                })
            
            # –Ø–∑—ã–∫–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            if language == 'java':
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–æ–∫ –≤ Java
                if stripped and stripped.endswith('{') and not stripped.startswith('//'):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Å—Ç—É–ø –ø–µ—Ä–µ–¥ {
                    if not re.match(r'^\s*.+\s*\{$', stripped):
                        style_issues.append({
                            'line': i,
                            'issue': 'Curly brace should be on the same line',
                            'severity': 'low'
                        })
            
            elif language == 'python':
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–±–æ–≤ vs –ø—Ä–æ–±–µ–ª–æ–≤ –≤ Python
                if '\t' in line:
                    style_issues.append({
                        'line': i,
                        'issue': 'Use spaces instead of tabs',
                        'severity': 'medium'
                    })
        
        return style_issues
	
    def analyze_test_coverage(self, file_path):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç–æ–≤–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ (–∏—â–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã)"""
        coverage = {
            'has_tests': False,
            'test_files': [],
            'coverage_estimate': 0
        }
        
        # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        file_dir = os.path.dirname(file_path)
        file_name = os.path.basename(file_path)
        file_base = os.path.splitext(file_name)[0]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        test_patterns = {
            'java': [f'**/test/**/*Test.java', f'**/*Test.java', f'**/{file_base}Test.java'],
            'python': [f'**/test_**.py', f'**/*_test.py', f'test_{file_base}.py'],
            'javascript': [f'**/*.test.js', f'**/*.spec.js', f'**/test/**/*.js']
        }
        
        language = self.detect_language(file_path)
        patterns = test_patterns.get(language, [])
        
        for pattern in patterns:
            test_files = glob.glob(pattern, root_dir=file_dir, recursive=True)
            coverage['test_files'].extend(test_files)
        
        coverage['has_tests'] = len(coverage['test_files']) > 0
        
        # –û—Ü–µ–Ω–æ—á–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ (–æ—á–µ–Ω—å –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
        if coverage['has_tests']:
            coverage['coverage_estimate'] = 70  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç—ã
        else:
            coverage['coverage_estimate'] = 0
        
        return coverage
	
    def export_complex_methods(self):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª"""
        complex_methods = []
        
        for file_path, metrics in self.stats.get('file_metrics', {}).items():
            for method in metrics.get('complex_methods', []):
                complex_methods.append({
                    'file': file_path,
                    'method': method['name'],
                    'lines': method['lines'],
                    'start_line': method['start_line']
                })
        
        if complex_methods:
            with open('complex_methods_report.txt', 'w', encoding='utf-8') as f:
                f.write("üî• –°–õ–û–ñ–ù–´–ï –ú–ï–¢–û–î–´ (–±–æ–ª–µ–µ 50 —Å—Ç—Ä–æ–∫)\n")
                f.write("=" * 60 + "\n\n")
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç—Ä–æ–∫ (—Å–∞–º—ã–µ —Å–ª–æ–∂–Ω—ã–µ —Å–Ω–∞—á–∞–ª–∞)
                complex_methods.sort(key=lambda x: x['lines'], reverse=True)
                
                for method in complex_methods:
                    f.write(f"üìÅ {method['file']}\n")
                    f.write(f"   üéØ {method['method']}() - {method['lines']} —Å—Ç—Ä–æ–∫ (—Å—Ç—Ä–æ–∫–∞ {method['start_line']})\n\n")
            
            print(f"üìÑ –û—Ç—á—ë—Ç –ø–æ —Å–ª–æ–∂–Ω—ã–º –º–µ—Ç–æ–¥–∞–º: complex_methods_report.txt")
            
    def find_important_comments(self, content, file_path):
        """–ù–∞—Ö–æ–¥–∏—Ç –í–°–ï –≤–∞–∂–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –Ω–µ —Ç–æ–ª—å–∫–æ TODO"""
        language = self.detect_language(file_path)
        
        # –ü–†–ê–í–ò–õ–¨–ù–û–ï –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ comment_patterns
        comment_patterns = self.todo_patterns.get('comment_patterns', {}).get(language, [])
        
        important_comments = []
        found_signatures = set()
        
        for pattern in comment_patterns:
            try:
                matches = re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE)
                for match in matches:
                    comment_text = match.group().strip()
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/—à–∞–±–ª–æ–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                    if self.is_trivial_comment(comment_text, language):
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—É—é —Å–∏–≥–Ω–∞—Ç—É—Ä—É
                    line_num = content[:match.start()].count('\n') + 1
                    comment_signature = f"{file_path}:{line_num}:{comment_text[:50]}"
                    
                    if comment_signature in found_signatures:
                        continue
                    found_signatures.add(comment_signature)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
                    comment_type = self.classify_comment(comment_text, language)
                    
                    if comment_type != 'TRIVIAL':
                        important_comments.append({
                            'type': comment_type,
                            'text': comment_text,
                            'line': line_num,
                            'language': language,
                            'file': file_path
                        })
                        
            except Exception as e:
                if self.config.get('verbose'):
                    print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ {file_path}: {e}")
        
        return important_comments
        
    def is_trivial_comment(self, comment_text, language):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–º"""
        trivial_patterns = [
            r'^\s*//\s*\w+\s*$',                    # // variable
            r'^\s*//\s*[A-Z][a-z]+\s+[A-Z][a-z]+',  # // Getter Setter  
            r'^\s*//\s*[A-Z][a-z]+$',               # // Constructor
            r'^\s*//\s*[a-z]+\s*$',                 # // todo
            r'^\s*/\*\s*\*/$',                      # /** */
            r'^\s*#\s*\w+\s*$',                     # # variable
        ]
        
        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        if len(comment_text.strip()) < self.config.get('minCommentLength', 25):
            return True
        
        # –®–∞–±–ª–æ–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        if any(re.match(pattern, comment_text) for pattern in trivial_patterns):
            return True
        
        # –ê–≤—Ç–æ-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        auto_generated = [
            'auto-generated', 'automatically generated', 'created by',
            'this code was generated', '@author', '@version'
        ]
        
        if any(marker in comment_text.lower() for marker in auto_generated):
            return True
        
        return False
        
    def is_trivial_comment(self, comment_text, language):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–º"""
        trivial_patterns = [
            r'^\s*//\s*\w+\s*$',                    # // variable
            r'^\s*//\s*[A-Z][a-z]+\s+[A-Z][a-z]+',  # // Getter Setter  
            r'^\s*//\s*[A-Z][a-z]+$',               # // Constructor
            r'^\s*//\s*[a-z]+\s*$',                 # // todo
            r'^\s*/\*\s*\*/$',                      # /** */
            r'^\s*#\s*\w+\s*$',                     # # variable
        ]
        
        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        if len(comment_text.strip()) < self.config.get('minCommentLength', 25):
            return True
        
        # –®–∞–±–ª–æ–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        if any(re.match(pattern, comment_text) for pattern in trivial_patterns):
            return True
        
        # –ê–≤—Ç–æ-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        auto_generated = [
            'auto-generated', 'automatically generated', 'created by',
            'this code was generated', '@author', '@version'
        ]
        
        if any(marker in comment_text.lower() for marker in auto_generated):
            return True
        
        return False
        
    def get_comment_patterns(self, language):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è —è–∑—ã–∫–∞"""
        comment_patterns = {
            'java': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'javascript': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'python': [r'#[^\n]*', r'""".*?"""', r"'''.*?'''"],
            'cpp': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'c': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'html': [r'<!--.*?-->'],
            'css': [r'/\*[\s\S]*?\*/'],
            'php': [r'//[^\n]*', r'#[^\n]*', r'/\*[\s\S]*?\*/'],
            'ruby': [r'#[^\n]*', r'=begin[\s\S]*?=end'],
            'go': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'rust': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'swift': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'kotlin': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'groovy': [r'//[^\n]*', r'/\*[\s\S]*?\*/'],
            'unknown': [r'//[^\n]*', r'/\*[\s\S]*?\*/', r'#[^\n]*']  # fallback
        }
        return comment_patterns.get(language, comment_patterns['unknown'])

def main():
    parser = argparse.ArgumentParser(description='CodeAnalyzer - –ê–Ω–∞–ª–∏–∑ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞')
    parser.add_argument('mode', nargs='?', default='compile', 
                       choices=['analyze', 'compile', 'split', 'clean', 'metrics'],
                       help='–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: analyze (—Ç–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑), compile (–∞–Ω–∞–ª–∏–∑ + –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ), split (—Ç–æ–ª—å–∫–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ), clean (–æ—á–∏—Å—Ç–∫–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤), metrics (—Ç–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫–∏)')
    parser.add_argument('--config', '-c', default='analyzer.yml',
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--dir', '-d', default='.',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    
    args = parser.parse_args()
    
    try:
        analyzer = CodeAnalyzer(args.config)
        
        if args.mode == 'analyze':
            analyzer.analyze_project(args.dir)
            analyzer.print_detailed_statistics()
        elif args.mode == 'compile':
            analyzer.compile_project(args.dir)
            analyzer.print_detailed_statistics()
        elif args.mode == 'split':
            analyzer.split_project(args.dir)
            analyzer.print_detailed_statistics()
        elif args.mode == 'clean':
            analyzer.clean_project(args.dir)
        elif args.mode == 'metrics':
            analyzer.analyze_project(args.dir)
            analyzer.print_quality_metrics()
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

